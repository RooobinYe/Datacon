#数据目录，约定读取挂载数据的目录
data_path = "/datacon2024/AI/CQ2"
model_path = "/datacon2024/AI/Models"

#具体的文件
q_file_path=f"{data_path}/Q.txt"
kb_file_path = f"{data_path}/KB.txt"
#输出答案的文件
output_file = "/result.csv"

import os
import csv
from FlagEmbedding import FlagModel

# Example for loading Embedding Model 
def load_Em_M_zh():
    # First: pip install -U FlagEmbedding
    sentences_1 = ["样例数据-1", "样例数据-2"]
    sentences_2 = ["样例数据-3", "样例数据-4"]
    
    model = FlagModel(os.path.join(model_path, 'bge-large-zh-v1.5'), 
                    query_instruction_for_retrieval="为这个句子生成表示以用于检索相关文章：",
                    use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation
    
    embeddings_1 = model.encode(sentences_1)
    embeddings_2 = model.encode(sentences_2)
    similarity = embeddings_1 @ embeddings_2.T
    print(similarity)



# Example for loading Embedding Model
def load_EM_M_en():
    sentences_1 = ["example data-1", "example data-2"]
    sentences_2 = ["example data-3", "example data-4"]
    model = FlagModel(os.path.join(model_path, 'bge-large-en-v1.5'), 
                    query_instruction_for_retrieval="为这个句子生成表示以用于检索相关文章：",
                    use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation
    embeddings_1 = model.encode(sentences_1)
    embeddings_2 = model.encode(sentences_2)
    similarity = embeddings_1 @ embeddings_2.T
    print(similarity)


# RAG(·)
def Mitigate_hallucination(Q_path, KB_path, output_path='./result.csv'):
    """
    Mitigates hallucination in answers generated by a Large Language Model (LLM) 
    by retrieving relevant context from a Knowledge Base (KB file) for queries in a given Q file.
    
    This function uses Retrieval-Augmented Generation (RAG) and Prompt Engineering 
    techniques to produce low-hallucination, high-quality answers from the LLM.

    :param Q_path: The path to the Q file containing queries.
    :type Q_path: str

    :param KB_path: The path to the Knowledge Base file.
    :type KB_path: str

    :param output_path: The path where the resulting CSV file will be saved.
    :type output_path: str, optional

    :return: None
    """

    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    try:
        with open(Q_path, 'r', encoding='utf-8') as input_file, \
             open(output_path, 'a', newline='', encoding='utf-8') as output_file:
                
                for idx, q in enumerate(input_file):
                    # (1) get Context from KB file via RAG
                    # ……

                    context = 'ccc'

                    # (2) generate LLM input
                    # ……

                    llm_input = 'ppp'

                    # (3) save as CSV file
                    writer = csv.writer(output_file)
                    if idx == 0:
                        writer.writerow(['Query', 'Context', 'Prompt'])
                    
                    else:
                        writer.writerow([q.strip(), context.strip(), llm_input.strip()])
                    
    except FileNotFoundError:
        print(f"The file {Q_path} does not exist.")
    except IOError as e:
        print(f"An I/O error occurred: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    else:
        print(f"File Q_star has been successfully generated at {output_path}")



if __name__ == "__main__": 

    Mitigate_hallucination( Q_path=q_file_path,
                            KB_path=kb_file_path,
                            output_path=output_file)
    
    load_Em_M_zh()
    load_EM_M_en()
    pass